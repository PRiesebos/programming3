{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Substitute assignment for programming 3\n",
    "\n",
    "Test code before I combine everything in a .py file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, countDistinct, count, avg, min, max, sum, when\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark GBFF parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GBFF Parser\") \\\n",
    "    .master(\"local[16]\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def parse_location(location):\n",
    "    \"\"\"Parse location string into start, end, strand\"\"\"\n",
    "    # Skip features starting with < or >\n",
    "    if location.startswith(('<', '>')):\n",
    "        return None, None, None\n",
    "    \n",
    "    strand = \"+\"\n",
    "    if \"complement\" in location:\n",
    "        strand = \"-\"\n",
    "        location = location.replace(\"complement(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    numbers = re.findall(r'\\d+', location)\n",
    "    \n",
    "    try:\n",
    "        return int(numbers[0]), int(numbers[1]), strand\n",
    "    except:\n",
    "        return None, None, strand\n",
    "    \n",
    "def extract_cds_counts(record):\n",
    "    \"\"\"Extracts CDS counts (with and without protein) from the COMMENT block.\"\"\"\n",
    "    comment_block = \" \".join(record)\n",
    "    \n",
    "    cds_with_protein_match = re.search(r'CDSs \\(with protein\\)\\s+::\\s+([\\d,]+)', comment_block)\n",
    "    cds_without_protein_match = re.search(r'CDSs \\(without protein\\)\\s+::\\s+([\\d,]+)', comment_block)\n",
    "\n",
    "    cds_with_protein = int(cds_with_protein_match.group(1).replace(',', '')) if cds_with_protein_match else None\n",
    "    cds_without_protein = int(cds_without_protein_match.group(1).replace(',', '')) if cds_without_protein_match else None\n",
    "    \n",
    "    return cds_with_protein, cds_without_protein\n",
    "\n",
    "def parse_gbff(file_path):\n",
    "    \"\"\"Parse a GBFF file into structured features\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "\n",
    "    records = []\n",
    "    current_record = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"LOCUS\"):\n",
    "            if current_record:\n",
    "                records.append(current_record)\n",
    "            current_record = [line]\n",
    "        elif line == \"//\":\n",
    "            current_record.append(line)\n",
    "            records.append(current_record)\n",
    "            current_record = []\n",
    "        else:\n",
    "            current_record.append(line)\n",
    "        if len(records) >= 20:  # Process first 20 genomes for testing\n",
    "            break\n",
    "\n",
    "    all_features = []\n",
    "    genomes_processed = 0\n",
    "    \n",
    "    for record in records:\n",
    "        try:\n",
    "            accession_line = next(line for line in record if line.startswith(\"ACCESSION\"))\n",
    "            primary_accession = accession_line.split()[1]\n",
    "            \n",
    "            organism = None\n",
    "            in_source = False\n",
    "            source_lines = []\n",
    "            \n",
    "            for line in record:\n",
    "                if line.startswith(\"     source\"):\n",
    "                    in_source = True\n",
    "                    source_lines.append(line)\n",
    "                elif in_source:\n",
    "                    if line.startswith(\" \" * 21):\n",
    "                        source_lines.append(line)\n",
    "                    else:\n",
    "                        in_source = False\n",
    "            \n",
    "            if source_lines:\n",
    "                source_text = \" \".join([line[21:] for line in source_lines])\n",
    "                organism_match = re.search(r'/organism=\"([^\"]+)\"', source_text)\n",
    "                if organism_match:\n",
    "                    organism = organism_match.group(1)\n",
    "\n",
    "            cds_with_protein, cds_without_protein = extract_cds_counts(record)\n",
    "\n",
    "        except (StopIteration, AttributeError):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            features_start = next(i for i, line in enumerate(record) if line.startswith(\"FEATURES\"))\n",
    "            valid_headers = {'ORIGIN', 'CONTIG', 'REFERENCE', 'COMMENT', '//'}\n",
    "            features_end = next((i for i in range(features_start+1, len(record)) \n",
    "                               if any(record[i].startswith(h) for h in valid_headers)), len(record))\n",
    "            features = record[features_start+1:features_end]\n",
    "        except StopIteration:\n",
    "            continue\n",
    "\n",
    "        current_feature = []\n",
    "        feature_blocks = []\n",
    "        for line in features:\n",
    "            if not line.startswith(\" \" * 21):\n",
    "                if current_feature:\n",
    "                    feature_blocks.append(current_feature)\n",
    "                current_feature = [line]\n",
    "            else:\n",
    "                current_feature.append(line)\n",
    "        if current_feature:\n",
    "            feature_blocks.append(current_feature)\n",
    "\n",
    "        for feature in feature_blocks:\n",
    "            first_line = feature[0]\n",
    "            feature_type = first_line[5:21].strip()\n",
    "            \n",
    "            if feature_type == \"gene\":\n",
    "                continue\n",
    "                \n",
    "            location_str = first_line[21:].strip()\n",
    "            start, end, strand = parse_location(location_str)\n",
    "            \n",
    "            if start is None or end is None:\n",
    "                continue\n",
    "                \n",
    "            length = end - start + 1 if end >= start else 0\n",
    "            \n",
    "            qual_text = \" \".join([line[21:] for line in feature[1:]])\n",
    "            qualifiers = {\n",
    "                'product': re.search(r'/product=\"([^\"]+)\"', qual_text),\n",
    "                'protein_id': re.search(r'/protein_id=\"([^\"]+)\"', qual_text),\n",
    "                'translation': re.search(r'/translation=\"([^\"]+)\"', qual_text)\n",
    "            }\n",
    "            \n",
    "            qualifiers = {k: v.group(1).replace('\\n', ' ') if v else None \n",
    "                         for k, v in qualifiers.items()}\n",
    "\n",
    "            all_features.append({\n",
    "                \"genome_id\": primary_accession,\n",
    "                \"organism\": organism,\n",
    "                \"feature_type\": feature_type,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"length\": length,\n",
    "                \"strand\": strand,\n",
    "                \"product\": qualifiers['product'],\n",
    "                \"protein_id\": qualifiers['protein_id'],\n",
    "                \"translation\": qualifiers['translation'],\n",
    "                \"cds_with_protein\": cds_with_protein,\n",
    "                \"cds_without_protein\": cds_without_protein\n",
    "            })\n",
    "\n",
    "        genomes_processed += 1\n",
    "        if genomes_processed >= 20:\n",
    "            break\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"genome_id\", StringType(), False),\n",
    "    StructField(\"organism\", StringType(), True),\n",
    "    StructField(\"feature_type\", StringType(), False),\n",
    "    StructField(\"start\", IntegerType(), False),\n",
    "    StructField(\"end\", IntegerType(), False),\n",
    "    StructField(\"length\", IntegerType(), False),\n",
    "    StructField(\"strand\", StringType(), False),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"protein_id\", StringType(), True),\n",
    "    StructField(\"translation\", StringType(), True),\n",
    "    StructField(\"cds_with_protein\", IntegerType(), True),\n",
    "    StructField(\"cds_without_protein\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Parse and create DataFrame\n",
    "file_path = \"/data/datasets/NCBI/refseq/ftp.ncbi.nlm.nih.gov/refseq/release/archaea/archaea.1.genomic.gbff\"\n",
    "parsed_data = parse_gbff(file_path)\n",
    "df = spark.createDataFrame(parsed_data, schema=schema)\n",
    "\n",
    "# Add coding_status column\n",
    "df = df.withColumn(\n",
    "    \"coding_status\",\n",
    "    when(col(\"feature_type\") == \"CDS\", \"coding\").otherwise(\"non-coding\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/31 01:19:07 WARN TaskSetManager: Stage 21 contains a task of very large size (34349 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/31 01:19:11 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 21 (TID 239): Attempting to kill Python Worker\n",
      "[Stage 19:=======================================>               (39 + 15) / 54]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+------------+-----+-----+------+------+----+------------------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+\n",
      "|genome_id      |organism|feature_type|start|end  |length|strand|gene|product                                   |protein_id    |translation                                                                                                                                                                                                                                                                            |coding_status|\n",
      "+---------------+--------+------------+-----+-----+------+------+----+------------------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+\n",
      "|NZ_SDJP01000015|NULL    |source      |1    |78255|78255 |+     |NULL|NULL                                      |NULL          |NULL                                                                                                                                                                                                                                                                                   |non-coding   |\n",
      "|NZ_SDJP01000015|NULL    |gene        |1    |460  |460   |+     |NULL|NULL                                      |NULL          |NULL                                                                                                                                                                                                                                                                                   |non-coding   |\n",
      "|NZ_SDJP01000015|NULL    |CDS         |1    |460  |460   |+     |NULL|halocyanin domain-containing protein      |WP_128905559.1|GGSGGGGGSDGSDGSDGGDGGSDGSDGSDGGSGGQEYLSEEPNY DGFLDDVSNYDGTVDMRDADEVTVDVGANDGLTFGPAAVAVSSGTTVVWEWVGQGGDH NVSGSDGSFESDTVGEEGHTFEYTFEESGTYTYVCTPHEAVGMKGAVYVE                                                                                                                             |coding       |\n",
      "|NZ_SDJP01000015|NULL    |gene        |543  |1370 |828   |+     |NULL|NULL                                      |NULL          |NULL                                                                                                                                                                                                                                                                                   |non-coding   |\n",
      "|NZ_SDJP01000015|NULL    |CDS         |543  |1370 |828   |+     |NULL|helix-turn-helix domain-containing protein|WP_128905560.1|MPQARLLVDLPDGPWIADVSRDFPDAGVRVLTAVPDESAGFALI RLTARDVDAVIAEMRDHGALSKVSVMARGDGVATVRIETTAPLLLVAAKRSGLPIEMP LDIEDGVAEVDVTGEHERVAALGKRFDEMGLDYEVERVRQRVDPVRLLTDRQQELLLA AVELGYYDVPRQSTLTEVADHVGIAKSTCSETLQRVERTVVREFVDDLPNRPLDDDEA VGAADAVPEATDEVDANAETDANTDGGADAETDANTDGGADAETDAEPAPSRPTYHS|coding       |\n",
      "+---------------+--------+------------+-----+-----+------+------+----+------------------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:===============================================>        (46 + 8) / 54]\r"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GBFF Parser\") \\\n",
    "    .master(\"local[16]\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def parse_location(location):\n",
    "    \"\"\"Parse location string into start, end, strand\"\"\"\n",
    "    strand = \"+\"\n",
    "    if \"complement\" in location:\n",
    "        strand = \"-\"\n",
    "        location = location.replace(\"complement(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    # Remove any special characters like < or >\n",
    "    location = re.sub(r'[<>]', '', location)\n",
    "    \n",
    "    # Handle join and other complex locations by taking first segment\n",
    "    if \"join\" in location:\n",
    "        location = location.split(\"(\")[1].split(\")\")[0].split(\",\")[0]\n",
    "    \n",
    "    try:\n",
    "        start, end = map(int, re.findall(r'\\d+', location)[:2])\n",
    "        return start, end, strand\n",
    "    except:\n",
    "        return None, None, strand\n",
    "\n",
    "def parse_gbff(file_path):\n",
    "    \"\"\"Parse a GBFF file into structured features\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "\n",
    "    records = []\n",
    "    current_record = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"LOCUS\"):\n",
    "            if current_record:\n",
    "                records.append(current_record)\n",
    "            current_record = [line]\n",
    "        elif line == \"//\":\n",
    "            current_record.append(line)\n",
    "            records.append(current_record)\n",
    "            current_record = []\n",
    "        else:\n",
    "            current_record.append(line)\n",
    "\n",
    "    all_features = []\n",
    "    for record in records:\n",
    "        try:\n",
    "            # Get primary accession and organism\n",
    "            accession_line = next(line for line in record if line.startswith(\"ACCESSION\"))\n",
    "            primary_accession = accession_line.split()[1]\n",
    "            \n",
    "            # Get organism from source feature\n",
    "            organism = None\n",
    "            for line in record:\n",
    "                if line.startswith(\"     source\") and \"/organism=\" in line:\n",
    "                    organism = re.search(r'/organism=\"([^\"]+)\"', line).group(1)\n",
    "                    break\n",
    "        except (StopIteration, AttributeError):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            features_start = next(i for i, line in enumerate(record) if line.startswith(\"FEATURES\"))\n",
    "            valid_headers = {'ORIGIN', 'CONTIG', 'REFERENCE', 'COMMENT', '//'}\n",
    "            features_end = next((i for i in range(features_start+1, len(record)) \n",
    "                               if any(record[i].startswith(h) for h in valid_headers)), len(record))\n",
    "            features = record[features_start+1:features_end]\n",
    "        except StopIteration:\n",
    "            continue\n",
    "\n",
    "        current_feature = []\n",
    "        feature_blocks = []\n",
    "        for line in features:\n",
    "            if not line.startswith(\" \" * 21):\n",
    "                if current_feature:\n",
    "                    feature_blocks.append(current_feature)\n",
    "                current_feature = [line]\n",
    "            else:\n",
    "                current_feature.append(line)\n",
    "        if current_feature:\n",
    "            feature_blocks.append(current_feature)\n",
    "\n",
    "        for feature in feature_blocks:\n",
    "            first_line = feature[0]\n",
    "            feature_type = first_line[5:21].strip()\n",
    "            location_str = first_line[21:].strip()\n",
    "            \n",
    "            start, end, strand = parse_location(location_str)\n",
    "            if start is None or end is None:\n",
    "                continue  # Skip invalid locations\n",
    "                \n",
    "            length = end - start + 1 if end >= start else 0\n",
    "            \n",
    "            # Parse qualifiers\n",
    "            qual_text = \" \".join([line[21:] for line in feature[1:]])\n",
    "            qualifiers = {\n",
    "                'gene': re.search(r'/gene=\"([^\"]+)\"', qual_text),\n",
    "                'product': re.search(r'/product=\"([^\"]+)\"', qual_text),\n",
    "                'protein_id': re.search(r'/protein_id=\"([^\"]+)\"', qual_text),\n",
    "                'translation': re.search(r'/translation=\"([^\"]+)\"', qual_text)\n",
    "            }\n",
    "            \n",
    "            # Clean qualifiers\n",
    "            qualifiers = {k: v.group(1).replace('\\n', ' ') if v else None \n",
    "                         for k, v in qualifiers.items()}\n",
    "\n",
    "            all_features.append({\n",
    "                \"genome_id\": primary_accession,\n",
    "                \"organism\": organism,\n",
    "                \"feature_type\": feature_type,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"length\": length,\n",
    "                \"strand\": strand,\n",
    "                \"gene\": qualifiers['gene'],\n",
    "                \"product\": qualifiers['product'],\n",
    "                \"protein_id\": qualifiers['protein_id'],\n",
    "                \"translation\": qualifiers['translation']\n",
    "            })\n",
    "\n",
    "    return all_features\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"genome_id\", StringType(), False),\n",
    "    StructField(\"organism\", StringType(), True),\n",
    "    StructField(\"feature_type\", StringType(), False),\n",
    "    StructField(\"start\", IntegerType(), False),\n",
    "    StructField(\"end\", IntegerType(), False),\n",
    "    StructField(\"length\", IntegerType(), False),\n",
    "    StructField(\"strand\", StringType(), False),\n",
    "    StructField(\"gene\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"protein_id\", StringType(), True),\n",
    "    StructField(\"translation\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Parse and create DataFrame\n",
    "file_path = \"/data/datasets/NCBI/refseq/ftp.ncbi.nlm.nih.gov/refseq/release/archaea/archaea.1.genomic.gbff\"\n",
    "parsed_data = parse_gbff(file_path)\n",
    "df = spark.createDataFrame(parsed_data, schema=schema)\n",
    "\n",
    "# Add coding_status column\n",
    "df = df.withColumn(\n",
    "    \"coding_status\",\n",
    "    when(col(\"feature_type\") == \"CDS\", \"coding\").otherwise(\"non-coding\")\n",
    ")\n",
    "\n",
    "# Show results\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/31 01:40:25 WARN TaskSetManager: Stage 24 contains a task of very large size (34349 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2935883"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:================================================>       (47 + 7) / 54]\r"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/31 01:46:55 WARN TaskSetManager: Stage 28 contains a task of very large size (34349 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 28:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|avg_features_per_genome|\n",
      "+-----------------------+\n",
      "|      80.76040491843864|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Question 1 - Hoeveel \"features\" heeft een Archaea genoom gemiddeld?\n",
    "\n",
    "# Group by genome_id and count features\n",
    "features_per_genome = df.groupBy(\"genome_id\").agg(count(\"*\").alias(\"feature_count\"))\n",
    "\n",
    "# Calculate average features per genome\n",
    "avg_features = features_per_genome.agg(avg(\"feature_count\").alias(\"avg_features_per_genome\"))\n",
    "\n",
    "# Show result\n",
    "avg_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/31 01:47:25 WARN TaskSetManager: Stage 34 contains a task of very large size (34349 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/31 01:47:27 WARN TaskSetManager: Stage 37 contains a task of very large size (34349 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 37:============================>                            (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coding to non-coding ratio: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Question 2 - Hoe is de verhouding tussen coding en non-coding features? (Deel coding door non-coding totalen).\n",
    "\n",
    "# Count coding and non-coding features\n",
    "coding_non_coding_counts = df.groupBy(\"coding_status\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Calculate ratio\n",
    "coding_count = coding_non_coding_counts.filter(col(\"coding_status\") == \"coding\").select(\"count\").collect()[0][0]\n",
    "non_coding_count = coding_non_coding_counts.filter(col(\"coding_status\") == \"non-coding\").select(\"count\").collect()[0][0]\n",
    "\n",
    "coding_ratio = coding_count / non_coding_count\n",
    "\n",
    "print(f\"Coding to non-coding ratio: {coding_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/31 01:53:12 WARN TaskSetManager: Stage 54 contains a task of very large size (34349 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 54:==========>                                             (3 + 13) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|min_proteins|max_proteins|\n",
      "+------------+------------+\n",
      "|     1018659|     1018659|\n",
      "+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# question 3 - Wat zijn de minimum en maximum aantal eiwitten van alle organismen in het file?\n",
    "\n",
    "# Compute min and max cds_with_protein per organism\n",
    "result = df.groupBy(\"organism\").agg(\n",
    "    min(\"cds_with_protein\").alias(\"min_proteins\"),\n",
    "    max(\"cds_with_protein\").alias(\"max_proteins\")\n",
    ")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/31 01:50:07 WARN TaskSetManager: Stage 40 contains a task of very large size (34349 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Question 4 - Verwijder alle non-coding (RNA) features en schrijf dit weg als een apart DataFrame (Spark format).\n",
    "\n",
    "coding_df = df.filter(col(\"coding_status\") == \"coding\")\n",
    "\n",
    "# Write to disk (e.g., Parquet)\n",
    "coding_df.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/homes/pcriesebos/Documents/programming3/Assignment6_alternative/dataframe_output_non_coding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/31 01:50:43 WARN TaskSetManager: Stage 41 contains a task of very large size (34349 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 41:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg_feature_length|\n",
      "+------------------+\n",
      "| 1313.002710939094|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Question 5 - Wat is de gemiddelde lengte van een feature ?\n",
    "\n",
    "# Calculate average feature length\n",
    "avg_feature_length = df.agg(avg(\"length\").alias(\"avg_feature_length\"))\n",
    "\n",
    "# Show result\n",
    "avg_feature_length.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store output\n",
    "Write the dataframe to snappy compressed parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/31 01:44:25 WARN TaskSetManager: Stage 27 contains a task of very large size (34349 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write as compressed Parquet\n",
    "\n",
    "df.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/homes/pcriesebos/Documents/programming3/Assignment6_alternative/dataframe_output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
